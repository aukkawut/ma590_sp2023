---
title: "$n$-Sample Test Classifier on Binary Outcomes of Stratified Randomized Experiments"
subtitle: "MA 590 Special Topics: Causal Inference"
author: "Aukkawut Ammartayakun"
institute: "Worcester Polytechinic Institute"
date: "`r format(Sys.time(), '%d %B, %Y')`"
bibliography: citation.bib
header-includes: 
    - \setbeamercolor{section name}{fg=draculabg}
output:   
    beamer_presentation:
        theme: "Ilmenau"
        colortheme: "dracula"
        highlight: breezedark
        slide_level: 2
        keep_tex: false
classoption: aspectratio=169
---
```{r setup, include=FALSE}
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  paste0("\n \\", "scriptsize","\n\n", x, "\n\n \\normalsize")
})
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
library(ggplot2)
```

# Two-Sample Case

Let say we have two group of data $D$: $D_t$ and $D_c$. We want to test whether there is a difference between the two groups. 

## Classifier Two-Sample Test [@c2st]

 - Combined two dataset into one dataset $D$.
 - Split the dataset into training and testing set.
 - Fit the classifier (like logistic regression) to the training set and predict the testing set.
 - Calculate the empirical loss $l_e$ of the classifier. If $|l_e - 0.5| < \epsilon$, then $\bar{\tau} = 0$.

```{r multiple_assignment, include=FALSE}
# Generic form
'%=%' = function(l, r, ...) UseMethod('%=%')

# Binary Operator
'%=%.lbunch' = function(l, r, ...) {
  Envir = as.environment(-1)

  if (length(r) > length(l))
    warning("RHS has more args than LHS. Only first", length(l), "used.")

  if (length(l) > length(r))  {
    warning("LHS has more args than RHS. RHS will be repeated.")
    r <- extendToMatch(r, l)
  }

  for (II in 1:length(l)) {
    do.call('<-', list(l[[II]], r[[II]]), envir=Envir)
  }
}

# Used if LHS is larger than RHS
extendToMatch <- function(source, destin) {
  s <- length(source)
  d <- length(destin)

  # Assume that destin is a length when it is a single number and source is not
  if(d==1 && s>1 && !is.null(as.numeric(destin)))
    d <- destin

  dif <- d - s
  if (dif > 0) {
    source <- rep(source, ceiling(d/s))[1:d]
  }
  return (source)
}

# Grouping the left hand side
g = function(...) {
  List = as.list(substitute(list(...)))[-1L]
  class(List) = 'lbunch'
  return(List)
}
```
```{r test_train_split, include=FALSE}
ttsplit <- function(X,y, p = 0.8){
    #test train split
    train <- sample(1:nrow(X), size = floor(p*nrow(X)), replace = FALSE) # 80% of data for training
    test <- setdiff(1:nrow(X), train) # 20% of data for testing
    g(X_train, X_test, y_train, y_test) %=% list(X[train,], X[test,], y[train], y[test])
    return(list(X_train = X_train, X_test = X_test, y_train = y_train, y_test = y_test))
}
```

```{r c2st, include=FALSE}
# based on https://gist.github.com/oddskool/409018f61d432f10fe00223e2b93cb51
c2st <- function(X, y, bf = 1000){
    # make sure X is dataframe
    X <- as.data.frame(X)
    # split data into training and testing sets
    g(X_train, X_test, y_train, y_test) %=% ttsplit(X, y)
    # fit logistic regression model
    model <- glm(y_train ~ ., data = X_train, family = binomial(link = "logit"))
    y_pred <- predict(model, X_test, type = "response")
    y_pred <- ifelse(y_pred > 0.5, 1, 0)
    emp_loss <- mean(y_pred != y_test)
    #p-val, 2 sided = 2*min(pval, 1-pval)
    #pval = P(T > t| H0)
    #T~Binomial(n, 0.5)
    #P(T > t| H0) = P(T>t and H0)/P(H0)

    #return(list(emp_loss = emp_loss, pval = pval))
    return(list(emp_loss = emp_loss))
}
```

## Testing the Algorithm

::: columns

:::: column
```{r}
set.seed(590)
# generate random multivariate gaussian data
n <- 1000
d <- 2
X <- matrix(rnorm(n*d), n, d)
y <- c(rep(0, n/2), rep(1, n/2))
c(c2st(X, y)$emp_loss)
```
::::

:::: column
```{r plot1, echo=FALSE}
# scatter plot of the data colored by the class 
# class 0: white, class 1: red
z <- c(rep(3, n/2), rep(4, n/2))
plot(X, col = z, pch = 19, cex = 0.2)
box(col="white")
axis(1, col="white", col.ticks="white", col.axis="white", cex.axis=1)
axis(2, col="white", col.ticks="white", col.axis="white", cex.axis=1)
mtext("axis 1", side=1, line=3, col="white", cex=1)
mtext("axis 2", side=2, line=3, col="white", cex=1)
```
::::

:::
```{=latex}
\pause
```
It is the *impossible* classification problem. Thus, the result should be close to near-chance level.

## Testing the Algorithm
::: columns

:::: column
```{r}
set.seed(590)
# generate two dataset: two gaussians
n <- 1000
d <- 2
X0 <- matrix(rnorm(n*d, 1,0.5), n, d)
X1 <- matrix(rnorm(n*d, -1,0.5), n, d)
# combine two dataset
X <- rbind(X0, X1)
y <- c(rep(0, n), rep(1, n))
c(c2st(X, y)$emp_loss)
```
::::

:::: column
```{r plot2, echo=FALSE}
# scatter plot of the data colored by the class 
# class 0: white, class 1: red
z <- c(rep(3, n), rep(4, n))
plot(X, col = z, pch = 19, cex = 0.2)
box(col="white")
axis(1, col="white", col.ticks="white", col.axis="white", cex.axis=1)
axis(2, col="white", col.ticks="white", col.axis="white", cex.axis=1)
mtext("axis 1", side=1, line=3, col="white", cex=1)
mtext("axis 2", side=2, line=3, col="white", cex=1)
```
::::

:::
```{=latex}
\pause
```
Now that it is not impossible, the test statistics will diverge from the near-chance level.

## Use C2ST on Causal Inference

* But, how can we use C2ST on causal inference?

## Example: 




# Generalization of $n$-Sample Case

## Possible Solution

 - Combine both treatment and control group within each stratum into one dataset $D^{s}_i$.
 - For each group, fit the classifier (like logistic regression) to the training set and predict the testing set.
 - Calculate the empirical loss $l_e$ of the classifier. If $|l_e - 0.5| < \epsilon$, then $\bar{\tau}_{\text{within}} = 0$
 - Find the way to infers $\bar{\tau}_{\text{between}}$

# References
